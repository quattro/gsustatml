\documentclass[12pt]{article}
\usepackage[left=72pt, right=72pt, top=72pt, bottom=72pt]{geometry}
\usepackage{enumerate}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsthm}

\newcommand{\bigpr}[1]{\Pr\Big[#1\Big]}
\newcommand{\bayes}[2]{\frac{\Pr[#2|#1] \cdot \Pr[#1]}{\Pr[#2]}}
\newcommand{\KL}[2]{\mathbb{KL}(#1||#2)}
\newcommand{\I}{\mathbb{I}}
\renewcommand{\H}{\mathbb{H}}
\newcommand{\D}{\mathcal{D}}

\begin{document}
\begin{center}
{\Large Chapter 3}\\
\today
\end{center}
\section*{Exercise 3.1}
\begin{proof}
Taking logs of equation 3.11 gives
\[\ell(\theta) = \log \Pr[\D|\theta] = \log\left(\theta^{N_1}(1 - \theta)^{N_0} \right) = N_1 \log \theta + N_0 \log (1 - \theta).\] 
Taking the derivative we get,
\[\frac{d \ell}{d \theta} = \frac{N_1}{\theta} - \frac{N_0}{1 - \theta}.\]
Setting equal to zero and solving we finally see (remember $N = N_1 + N_0$),
\begin{align*}
    \frac{N_1}{\theta} - \frac{N_0}{1 - \theta} &= 0\\
    N_1 (1 - \theta) - N_0 \theta &= N_1 - \theta(N_1 + N_0)\\
    &= N_1 - N\theta.\\
    \iff \theta = \frac{N_1}{N}.
\end{align*}
\end{proof}
\section*{Exercise 3.9}
\begin{proof}
By definition we have $\Pr[\theta | \D]  = \Pr[\D, \theta] / \Pr[\D]$.
As $m = \max (\D, b)$ we have that $m \geq b$. Therefore we consider two cases:
\begin{description}
    \item[case 1: $m = b$]
        \begin{align*}
            \frac{Kb^K}{\theta^{N + K + 1}} \cdot \left(\frac{K}{(N + K)b^N}\right)^{-1} \I[\theta \geq m]
            = \frac{b^{K + N}(N+K)}{\theta^{N + K + 1}} \I[\theta \geq m]\\
            = \frac{m^{K + N}(N+K)}{\theta^{N + K + 1}} \I[\theta \geq m]
            = \text{Pareto}[\theta | N + K, m]
        \end{align*}
    \item[case 2: $m > b$]
        \begin{align*}
            \frac{Kb^K}{\theta^{N + K + 1}} \cdot \left(\frac{Kb^K}{(N + K)m^N}\right)^{-1} \I[\theta \geq m]
            = \frac{m^{K + N}(N+K)}{\theta^{N + K + 1}} \I[\theta \geq m]\\
            = \text{Pareto}[\theta | N + K, m]
        \end{align*}
\end{description}
In both cases we get the same result, therefore we have
$\Pr[\theta|\D] = \text{Pareto}[\theta| N + K, m]$.  \end{proof}
\section*{Exercise 3.11}
\begin{proof}
Likelihood of a single lifetime variable is defined as $\Pr[x | \theta] = \theta e^{-\theta x}.$
We are give data $\D = \{x_i\}_i^n$, so the data-likelihood is (due to
independence of machine lifetimes)
\[\Pr[\D|\theta] = \Pr[x_1, \dotsc, x_n|\theta] = \prod_{i=1}^n \Pr[x_i | \theta] =
    \theta^n \exp(-\theta \sum_{i=1}^n x_i).\]
Taking the derivative of this function we have
\[\Pr[\D|\theta]' = n \theta^{n - 1}\exp(-\theta \sum_{i=1}^n x_i)
    - \theta^n \exp(-\theta \sum_{i=1}^n x_i) \sum_{i=1}^n x_i.\]
Setting this equal to 0 and solving for $\theta$ we see,
\begin{align*}
    n \theta^{n - 1}\exp(-\theta \sum_{i=1}^n x_i) = 
        \theta^n \exp(-\theta \sum_{i=1}^n x_i) \sum_{i=1}^n x_i \iff\\
    n = \theta \sum_{i=1}^n x_i \iff \theta =
        \frac{n}{\sum_{i=1}^n x_i} = \frac{1}{1/n \sum_{i=1}^n x_i} = \frac{1}{\bar{x}}.
\end{align*}
Therefore the MLE $\hat{\theta}$ for $\Pr[\D|\theta]$ is $1/\bar{x}.$
\end{proof}
\section*{Exercise 3.13}
\end{document}
